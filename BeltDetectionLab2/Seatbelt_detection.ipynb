{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7f90c28",
   "metadata": {},
   "outputs": [],
   "source": [
    "from os.path import dirname, abspath\n",
    "import os\n",
    "import cv2\n",
    "from contextlib import contextmanager\n",
    "import numpy as np\n",
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d94a9b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "VIDEO = \"test.mp4\"\n",
    "WEIGHTS = \"YOLOFI2.weights\"\n",
    "CONFIG = \"YOLOFI.cfg\"\n",
    "OBJ_NAMES = \"obj.names\"\n",
    "SAVE_PATH = os.getcwd() + \"/\"\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6501659f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#class define\n",
    "\n",
    "class BeltVisible:\n",
    "    # lists of frames (ids) where the belt part is supposed to be detected as closed\n",
    "    # first frame has id 0\n",
    "    def __init__(self, belt_frames, belt_corner_frames):\n",
    "        self.belt_frames = belt_frames\n",
    "        self.belt_corner_frames = belt_corner_frames\n",
    "\n",
    "\n",
    "class BeltDetected:\n",
    "    # list of frames (ids) where the belt part was detected as closed\n",
    "    # first frame has id 0\n",
    "    def __init__(self):\n",
    "        self.belt_frames = []  # main part\n",
    "        self.belt_corner_frames = []  # corner part\n",
    "\n",
    "    def add_belt(self, frame):\n",
    "        self.belt_frames.append(frame)\n",
    "\n",
    "    def add_corner_belt(self, frame):\n",
    "        self.belt_corner_frames.append(frame)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d8d831f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def video_capture(*args, **kwargs):\n",
    "    cap = cv2.VideoCapture(*args, **kwargs)\n",
    "    try:\n",
    "        yield cap\n",
    "    finally:\n",
    "        cap.release()\n",
    "        cv2.destroyAllWindows()\n",
    "\n",
    "\n",
    "def get_layers(net):\n",
    "    layer_names = net.getLayerNames()\n",
    "    return [layer_names[layer[0] - 1] for layer in net.getUnconnectedOutLayers()]\n",
    "\n",
    "\n",
    "def get_classes():\n",
    "    classes = []\n",
    "    with open(OBJ_NAMES, \"r\") as file:\n",
    "        classes = [line.strip() for line in file.readlines()]\n",
    "    return classes\n",
    "\n",
    "\n",
    "def belt_detector(net, img, belt_detected, current_frame):\n",
    "    blob = cv2.dnn.blobFromImage(img, 0.00392, (480, 480), (0, 0, 0), True, crop=False)\n",
    "    net.setInput(blob)\n",
    "\n",
    "    height, width, channels = img.shape\n",
    "\n",
    "    outs = net.forward(get_layers(net))\n",
    "    for out in outs:\n",
    "        for detection in out:\n",
    "            scores = detection[5:]\n",
    "            class_id = np.argmax(scores)\n",
    "            confidence = scores[class_id]\n",
    "\n",
    "            if confidence > 0.2:\n",
    "                center_x = int(detection[0] * width)\n",
    "                center_y = int(detection[1] * height)\n",
    "                w = int(detection[2] * width)\n",
    "                h = int(detection[3] * height)\n",
    "                x = int(center_x - w / 2)\n",
    "                y = int(center_y - h / 2)\n",
    "                cv2.rectangle(img, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
    "                if class_id == 1:\n",
    "                    belt_detected.add_corner_belt(current_frame)\n",
    "                elif class_id == 0:\n",
    "                    belt_detected.add_belt(current_frame)\n",
    "\n",
    "    return belt_detected\n",
    "\n",
    "\n",
    "def print_belt_report(belt_detected, total_frames):\n",
    "    belt_visible = BeltVisible(\n",
    "        belt_frames=[i for i in range(125)],\n",
    "        belt_corner_frames=[i for i in range(125)]\n",
    "    )\n",
    "    success_belt_frames = set(belt_visible.belt_frames).intersection(belt_detected.belt_frames)\n",
    "    success_belt_corner_frames = set(belt_visible.belt_corner_frames).intersection(\n",
    "        belt_detected.belt_corner_frames\n",
    "    )\n",
    "\n",
    "    logging.info(\n",
    "        \"Total frames {}, successfully detected belt {} of {} times, corner belt - {} of {}\".format(\n",
    "            total_frames,\n",
    "            len(success_belt_frames),\n",
    "            len(belt_visible.belt_frames),\n",
    "            len(success_belt_corner_frames),\n",
    "            len(belt_visible.belt_corner_frames)\n",
    "        ))\n",
    "    logging.info(\"Non detected belt frames: {}\".format(\n",
    "        set(belt_visible.belt_frames).difference(belt_detected.belt_frames))\n",
    "    )\n",
    "    logging.info(\"False detected belt frames: {}\".format(\n",
    "        set(belt_detected.belt_frames).difference(belt_visible.belt_frames))\n",
    "    )\n",
    "    logging.info(\"Non detected belt corner frames: {}\".format(\n",
    "        set(belt_visible.belt_corner_frames).difference(belt_detected.belt_corner_frames))\n",
    "    )\n",
    "    logging.info(\"False detected belt corner frames: {}\".format(\n",
    "        set(belt_detected.belt_corner_frames).difference(belt_visible.belt_corner_frames))\n",
    "    )\n",
    "\n",
    "\n",
    "def apply_clahe(img, **kwargs):\n",
    "    lab = cv2.cvtColor(img, cv2.COLOR_BGR2LAB)\n",
    "    lab = cv2.split(lab)\n",
    "    clahe = cv2.createCLAHE(**kwargs)\n",
    "    lab[0] = clahe.apply(lab[0])\n",
    "    lab = cv2.merge((lab[0], lab[1], lab[2]))\n",
    "    return cv2.cvtColor(lab, cv2.COLOR_LAB2BGR)\n",
    "\n",
    "\n",
    "def apply_gabor(img, **kwargs):\n",
    "    g_kernel = cv2.getGaborKernel(**kwargs)\n",
    "    return cv2.filter2D(img, cv2.CV_8UC3, g_kernel.sum())\n",
    "\n",
    "\n",
    "def increase_brightness(img):\n",
    "    hsv = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)\n",
    "    h, s, v = cv2.split(hsv)\n",
    "    v += 255\n",
    "    final_hsv = cv2.merge((h, s, v))\n",
    "    return cv2.cvtColor(final_hsv, cv2.COLOR_HSV2BGR)\n",
    "\n",
    "\n",
    "def build_filters():\n",
    "    filters = []\n",
    "    ksize = 31\n",
    "    for theta in np.arange(0, np.pi, np.pi / 16):\n",
    "        kern = cv2.getGaborKernel((ksize, ksize), 0.3, theta, 9.0, 0.6, 50, ktype=cv2.CV_32F)\n",
    "    kern /= 1.5*kern.sum()\n",
    "    filters.append(kern)\n",
    "    return filters\n",
    "\n",
    "def process(img, filters):\n",
    "    accum = np.zeros_like(img)\n",
    "    for kern in filters:\n",
    "        fimg = cv2.filter2D(img, cv2.CV_8UC3, kern)\n",
    "    np.maximum(accum, fimg, accum)\n",
    "    return accum\n",
    "\n",
    "def main():\n",
    "    with video_capture(VIDEO) as cap:\n",
    "        net = cv2.dnn.readNet(WEIGHTS, CONFIG)\n",
    "        frame_id = -1\n",
    "        belt_detected = BeltDetected()\n",
    "        while True:\n",
    "            frame = cap.read()\n",
    "            frame_id += 1\n",
    "            if not frame[0]:\n",
    "                break\n",
    "            img = frame[1][:, 50: -50]\n",
    "\n",
    "            # TODO: your code here\n",
    "            img = increase_brightness(img)\n",
    "\n",
    "            img = apply_clahe(img=img, clipLimit=5, tileGridSize=(17, 17))\n",
    "            # better results for corner belt, slightly worse results for main part\n",
    "            # img = apply_gabor(img=img, ksize=(4, 4), sigma=5, theta=89,\n",
    "            #                   lambd=1, gamma=2, psi=0, ktype=cv2.CV_64F)\n",
    "            # the best result for main part\n",
    "            img = apply_gabor(img=img, ksize=(31, 31), sigma=2.9, theta=160,\n",
    "                              lambd=14.5, gamma=35, psi=50, ktype=cv2.CV_64F)\n",
    "            belt_detected = belt_detector(net, img, belt_detected, frame_id)\n",
    "            cv2.imshow(\"Image\", img)\n",
    "\n",
    "            # to decide which frame should be assumed as belt position changing\n",
    "            # chosen id=124, although it is arguable\n",
    "            # if frame_id in range(120, 126):\n",
    "            #     cv2.imwrite(SAVE_PATH + \"{id}.png\".format(id=frame_id), img)\n",
    "\n",
    "            key = cv2.waitKey(1)\n",
    "            if key == 27:\n",
    "                break\n",
    "        print_belt_report(belt_detected, frame_id)\n",
    "\n",
    "    \n",
    "    net =cv2.dnn.readNet(\"YOLOFI2.weights\",\"YOLOFI.cfg\")\n",
    "    cap = cv2.VideoCapture(\"test.mp4\")\n",
    "    classes=[]  \n",
    "    l=1\n",
    "    with open(\"obj.names\",\"r\")as f:\n",
    "            classes = [line.strip()for line in f.readlines()]\n",
    "            layers_names = net.getLayerNames()\n",
    "            outputlayers= [layers_names[i[0]-1]for i in net.getUnconnectedOutLayers()]\n",
    "            colors = np.random.uniform(0,255,size =(len(classes),3))\n",
    "            font = cv2.FONT_HERSHEY_PLAIN\n",
    "            frame_id=0  \n",
    "            dd =-1\n",
    "            time_now=time.time()\n",
    "            frame_id=0\n",
    "            err=0\n",
    "\n",
    "            count = 0 # for counting frames\n",
    "\n",
    "            while True:\n",
    "                _, frame = cap.read()\n",
    "                frame_id += 1           \n",
    "                beltcornerdetected = False\n",
    "                beltdetected = False \n",
    "                height , width , channels = frame.shape\n",
    "\n",
    "\n",
    "                #Type you code here\n",
    "\n",
    "                clahe = cv2.createCLAHE(clipLimit=5.0, tileGridSize=(8,8))\n",
    "\n",
    "                R, G, B = cv2.split(frame)\n",
    "\n",
    "\n",
    "                output1_R = clahe.apply(R)\n",
    "                output1_G = clahe.apply(G)\n",
    "                output1_B = clahe.apply(B)\n",
    "\n",
    "                frame = cv2.merge((output1_R, output1_G, output1_B))\n",
    "\n",
    "                cv2.fastNlMeansDenoising(frame,frame,3,5,11)\n",
    "\n",
    "                filters = build_filters()\n",
    "                frame = process(frame, filters)\n",
    "\n",
    "                blob = cv2.dnn.blobFromImage(frame, 0.00392, (480,480),(0,0,0),True,crop= False)\n",
    "                net.setInput(blob)\n",
    "                outs = net.forward(outputlayers)\n",
    "                class_ids=[]\n",
    "                boxes=[]\n",
    "                shape=[]\n",
    "                confidence=0\n",
    "\n",
    "                for out in outs:\n",
    "\n",
    "                    for detection in out:\n",
    "                        scores = detection[5:]\n",
    "                        class_id = np.argmax(scores)\n",
    "                        confidence = scores[class_id]\n",
    "                        \n",
    "                        if confidence> 0.2:\n",
    "                            center_x= int(detection[0] *width)\n",
    "                            center_y=int(detection[1]* height)\n",
    "                            w= int(detection[2] *width)\n",
    "                            h= int(detection[3] * height)\n",
    "                            x= int(center_x- w /2)\n",
    "                            y= int(center_y -h /2)\n",
    "                            cv2.rectangle(frame,(x,y),(x+w,y+h),(0,255,0),2)\n",
    "                            if class_id== 1:\n",
    "                                beltcornerdetected=True\n",
    "                            elif class_id == 0:\n",
    "                                beltdetected=True\n",
    "                            \n",
    "                print(count, ' ', beltdetected)\n",
    "                count+=1\n",
    "                cv2.imshow(\"Image\",frame)\n",
    "                key =cv2.waitKey(1)\n",
    "                if key == 27:\n",
    "                  break\n",
    "           \n",
    "            cap.release()    \n",
    "            cv2.destroyAllWindows()\n",
    "            \n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c06f175d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] [--video VIDEO]\n",
      "                             [--loglevel {debug,info,warning,error}]\n",
      "ipykernel_launcher.py: error: unrecognized arguments: --f=/Users/jaylong/Library/Jupyter/runtime/kernel-v3272ae5d6da9d872b8d94c73fe3d7d7dc7a23042b.json\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jaylong/Library/Python/3.9/lib/python/site-packages/IPython/core/interactiveshell.py:3516: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "seatbelt_detection.py\n",
    "---------------------------------\n",
    "Real‑time driver‑seat‑belt detector (YOLOv3 / OpenCV‑DNN).\n",
    "\n",
    "依賴：\n",
    "    pip install opencv-python numpy\n",
    "    # 建議先安裝和 GPU 相容版本的 OpenCV /  CUDA / cuDNN，可大幅提升 FPS\n",
    "\n",
    "用法：\n",
    "    python seatbelt_detection.py               # 讀預設攝影機 0\n",
    "    python seatbelt_detection.py --video test.mp4\n",
    "    python seatbelt_detection.py --video 0     # 同上，顯式指定 webcam\n",
    "\"\"\"\n",
    "\n",
    "import argparse\n",
    "import contextlib\n",
    "import logging\n",
    "import os\n",
    "import time\n",
    "from pathlib import Path\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# ======== 路徑設定（請視需要修改） =========================================\n",
    "ROOT_DIR = Path(os.getcwd())\n",
    "WEIGHTS = str(ROOT_DIR / \"YOLOFI2.weights\")\n",
    "CONFIG  = str(ROOT_DIR / \"YOLOFI.cfg\")\n",
    "OBJ_NAMES = str(ROOT_DIR / \"obj.names\")\n",
    "# ===========================================================================\n",
    "\n",
    "\n",
    "# ---------- 資料結構 --------------------------------------------------------\n",
    "class BeltDetected:\n",
    "    \"\"\"累積模型偵測到安全帶的影格索引\"\"\"\n",
    "    def __init__(self):\n",
    "        self.belt_frames = set()\n",
    "        self.belt_corner_frames = set()\n",
    "\n",
    "    def add_belt(self, frame_id: int):\n",
    "        self.belt_frames.add(frame_id)\n",
    "\n",
    "    def add_corner_belt(self, frame_id: int):\n",
    "        self.belt_corner_frames.add(frame_id)\n",
    "\n",
    "\n",
    "class BeltVisible:\n",
    "    \"\"\"\n",
    "    預期在 ground‑truth 中看得到安全帶／轉角帶的影格清單\n",
    "    這裡放示範值：0‑124 frame 皆應可見；實務請改成自己的標註。\n",
    "    \"\"\"\n",
    "    def __init__(self, belt_frames=None, belt_corner_frames=None):\n",
    "        self.belt_frames = belt_frames or []\n",
    "        self.belt_corner_frames = belt_corner_frames or []\n",
    "\n",
    "\n",
    "# ---------- 工具函式 --------------------------------------------------------\n",
    "@contextlib.contextmanager\n",
    "def video_capture(src):\n",
    "    \"\"\"確保離開時能自動釋放資源與關閉所有 OpenCV 視窗\"\"\"\n",
    "    cap = cv2.VideoCapture(src)\n",
    "    try:\n",
    "        yield cap\n",
    "    finally:\n",
    "        cap.release()\n",
    "        cv2.destroyAllWindows()\n",
    "\n",
    "\n",
    "def get_layers(net):\n",
    "    layer_names = net.getLayerNames()\n",
    "    return [layer_names[i[0] - 1] for i in net.getUnconnectedOutLayers()]\n",
    "\n",
    "\n",
    "def get_classes():\n",
    "    with open(OBJ_NAMES, \"r\", encoding=\"utf-8\") as f:\n",
    "        return [line.strip() for line in f.readlines()]\n",
    "\n",
    "\n",
    "# ---------- 影像增強 --------------------------------------------------------\n",
    "def increase_brightness(img, delta: int = 60):\n",
    "    hsv = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)\n",
    "    h, s, v = cv2.split(hsv)\n",
    "    v = np.clip(v.astype(np.int16) + delta, 0, 255).astype(np.uint8)\n",
    "    final_hsv = cv2.merge((h, s, v))\n",
    "    return cv2.cvtColor(final_hsv, cv2.COLOR_HSV2BGR)\n",
    "\n",
    "\n",
    "def apply_clahe(img, clip_limit=5.0, grid_size=(8, 8)):\n",
    "    lab = cv2.cvtColor(img, cv2.COLOR_BGR2LAB)\n",
    "    l, a, b = cv2.split(lab)\n",
    "    clahe = cv2.createCLAHE(clipLimit=clip_limit, tileGridSize=grid_size)\n",
    "    l = clahe.apply(l)\n",
    "    lab = cv2.merge((l, a, b))\n",
    "    return cv2.cvtColor(lab, cv2.COLOR_LAB2BGR)\n",
    "\n",
    "\n",
    "def build_gabor_filters(ksize=31):\n",
    "    filters = []\n",
    "    for theta in np.arange(0, np.pi, np.pi / 16):\n",
    "        kern = cv2.getGaborKernel(\n",
    "            (ksize, ksize), 0.3, theta, 9.0, 0.6, 50, ktype=cv2.CV_32F\n",
    "        )\n",
    "        kern /= 1.5 * kern.sum()\n",
    "        filters.append(kern)\n",
    "    return filters\n",
    "\n",
    "\n",
    "def apply_gabor_bank(img, filters):\n",
    "    accum = np.zeros_like(img)\n",
    "    for kern in filters:\n",
    "        fimg = cv2.filter2D(img, cv2.CV_8UC3, kern)\n",
    "        np.maximum(accum, fimg, accum)  # element‑wise max\n",
    "    return accum\n",
    "\n",
    "\n",
    "def preprocess_frame(frame, filters):\n",
    "    \"\"\"可視需要刪改；此處做亮度提升 + CLAHE + Gabor 濾波\"\"\"\n",
    "    frame = increase_brightness(frame, delta=60)\n",
    "    frame = apply_clahe(frame, clip_limit=5.0, grid_size=(8, 8))\n",
    "    frame = apply_gabor_bank(frame, filters)\n",
    "    return frame\n",
    "\n",
    "\n",
    "# ---------- YOLO 偵測 -------------------------------------------------------\n",
    "def belt_detector(net, img, belt_detected: BeltDetected, frame_idx: int,\n",
    "                  conf_thres=0.2):\n",
    "    blob = cv2.dnn.blobFromImage(\n",
    "        img, 0.00392, (480, 480), (0, 0, 0), swapRB=True, crop=False\n",
    "    )\n",
    "    net.setInput(blob)\n",
    "    outs = net.forward(get_layers(net))\n",
    "\n",
    "    h, w = img.shape[:2]\n",
    "\n",
    "    for out in outs:\n",
    "        for det in out:\n",
    "            scores = det[5:]\n",
    "            cls_id = int(np.argmax(scores))\n",
    "            confidence = scores[cls_id]\n",
    "            if confidence < conf_thres:\n",
    "                continue\n",
    "\n",
    "            # 轉回影像座標\n",
    "            cx, cy, bw, bh = det[0:4] * np.array([w, h, w, h])\n",
    "            x, y = int(cx - bw / 2), int(cy - bh / 2)\n",
    "            bw, bh = int(bw), int(bh)\n",
    "\n",
    "            # 畫框\n",
    "            color = (0, 255, 0) if cls_id == 0 else (255, 0, 0)\n",
    "            cv2.rectangle(img, (x, y), (x + bw, y + bh), color, 2)\n",
    "            label = f\"{cls_id}:{confidence:.2f}\"\n",
    "            cv2.putText(img, label, (x, y - 6),\n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 0.6, color, 2)\n",
    "\n",
    "            # 累積偵測結果\n",
    "            if cls_id == 1:          # 假設 1 = seatbelt corner\n",
    "                belt_detected.add_corner_belt(frame_idx)\n",
    "            elif cls_id == 0:        # 假設 0 = seatbelt\n",
    "                belt_detected.add_belt(frame_idx)\n",
    "\n",
    "    return belt_detected\n",
    "\n",
    "\n",
    "# ---------- 統計報表 --------------------------------------------------------\n",
    "def print_belt_report(belt_detected: BeltDetected, total_frames: int):\n",
    "    # 示範：假設前 125 幀理論上都應偵測到\n",
    "    belt_visible = BeltVisible(\n",
    "        belt_frames=list(range(125)),\n",
    "        belt_corner_frames=list(range(125)),\n",
    "    )\n",
    "\n",
    "    succ_belt = set(belt_visible.belt_frames) & belt_detected.belt_frames\n",
    "    succ_corner = set(belt_visible.belt_corner_frames) & belt_detected.belt_corner_frames\n",
    "\n",
    "    logging.info(\n",
    "        \"Total frames: %d | seat‑belt: %d / %d | corner: %d / %d\",\n",
    "        total_frames, len(succ_belt), len(belt_visible.belt_frames),\n",
    "        len(succ_corner), len(belt_visible.belt_corner_frames)\n",
    "    )\n",
    "    logging.debug(\"Missed belt frames: %s\",\n",
    "                  set(belt_visible.belt_frames) - belt_detected.belt_frames)\n",
    "    logging.debug(\"False‑positive belt frames: %s\",\n",
    "                  belt_detected.belt_frames - set(belt_visible.belt_frames))\n",
    "    logging.debug(\"Missed corner frames: %s\",\n",
    "                  set(belt_visible.belt_corner_frames) - belt_detected.belt_corner_frames)\n",
    "    logging.debug(\"False‑positive corner frames: %s\",\n",
    "                  belt_detected.belt_corner_frames - set(belt_visible.belt_corner_frames))\n",
    "\n",
    "\n",
    "# ---------- 主程式 ----------------------------------------------------------\n",
    "def parse_args():\n",
    "    ap = argparse.ArgumentParser(description=\"Seat‑belt detector (YOLOv3 / OpenCV‑DNN)\")\n",
    "    ap.add_argument(\"--video\", type=str, default=\"0\",\n",
    "                    help=\"影片路徑，或 camera index (預設 0)\")\n",
    "    ap.add_argument(\"--loglevel\", type=str, default=\"info\",\n",
    "                    choices=[\"debug\", \"info\", \"warning\", \"error\"])\n",
    "    return ap.parse_args()\n",
    "\n",
    "\n",
    "def main():\n",
    "    args = parse_args()\n",
    "    logging.basicConfig(\n",
    "        format=\"%(asctime)s │ %(levelname)-7s │ %(message)s\",\n",
    "        level=getattr(logging, args.loglevel.upper(), logging.INFO),\n",
    "    )\n",
    "\n",
    "    # 讀取類別與模型\n",
    "    classes = get_classes()\n",
    "    net = cv2.dnn.readNet(WEIGHTS, CONFIG)\n",
    "\n",
    "    # 若有 GPU / CUDA，可取消下行註解\n",
    "    # net.setPreferableBackend(cv2.dnn.DNN_BACKEND_CUDA)\n",
    "    # net.setPreferableTarget(cv2.dnn.DNN_TARGET_CUDA_FP16)\n",
    "\n",
    "    filters = build_gabor_filters()\n",
    "    belt_detected = BeltDetected()\n",
    "\n",
    "    # 相機或影片\n",
    "    src = int(args.video) if args.video.isdigit() else args.video\n",
    "\n",
    "    with video_capture(src) as cap:\n",
    "        frame_idx = -1\n",
    "        fps_time = time.time()\n",
    "        while True:\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "            frame_idx += 1\n",
    "\n",
    "            # 可視需求裁切 ROI，例如只留中段避免誤判\n",
    "            frame_roi = frame[:, 50:-50]\n",
    "\n",
    "            # 圖像增強\n",
    "            frame_proc = preprocess_frame(frame_roi, filters)\n",
    "\n",
    "            # YOLO 偵測\n",
    "            belt_detected = belt_detector(net, frame_proc, belt_detected, frame_idx)\n",
    "\n",
    "            # 顯示 FPS\n",
    "            dt = time.time() - fps_time\n",
    "            fps = 1.0 / dt if dt > 0 else 0\n",
    "            fps_time = time.time()\n",
    "            cv2.putText(frame_proc, f\"FPS:{fps:.1f}\", (10, 30),\n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 255, 255), 2)\n",
    "\n",
    "            cv2.imshow(\"Seat‑belt Detection\", frame_proc)\n",
    "            if cv2.waitKey(1) & 0xFF == 27:    # ESC 離開\n",
    "                break\n",
    "\n",
    "    print_belt_report(belt_detected, frame_idx)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
